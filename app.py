"""ãƒ©ã‚¤ãƒ–é…ä¿¡ãƒãƒ£ãƒƒãƒˆåˆ†æãƒ„ãƒ¼ãƒ« - Streamlitãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒª"""
import streamlit as st
import tempfile
import os
import pickle
import glob
import time
import base64
import re
import pandas as pd
from datetime import datetime
from typing import Dict, Optional
from utils.csv_processor import (
    load_csv,
    validate_and_process_data,
    extract_questions,
    load_csv_with_elapsed_time
)
from utils.ai_analyzer import analyze_all_comments
from utils.google_sheets import (
    calculate_statistics,
    calculate_question_statistics
)
from utils.api_key_manager import render_api_key_input
from config import COMPANIES, DEFAULT_COMPANY, get_company_config


def inject_custom_css():
    """ã‚«ã‚¹ã‚¿ãƒ CSSã‚’æ³¨å…¥"""
    css_file_path = os.path.join(os.path.dirname(__file__), "styles", "custom.css")

    if os.path.exists(css_file_path):
        with open(css_file_path, "r", encoding="utf-8") as f:
            css = f.read()
        st.markdown(f"<style>{css}</style>", unsafe_allow_html=True)


def remove_live_name_from_filename(filename: str) -> str:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã€Œ_(ãƒ©ã‚¤ãƒ–é…ä¿¡ã®åå‰)ã€ã®éƒ¨åˆ†ã‚’å‰Šé™¤
    
    Args:
        filename: å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«å
        
    Returns:
        ãƒ©ã‚¤ãƒ–é…ä¿¡åã‚’å‰Šé™¤ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«å
    """
    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: _(...) ã¾ãŸã¯ _(...)ï¼ˆåŠè§’æ‹¬å¼§ã®ã¿ï¼‰
    filename = re.sub(r'_\s*\([^)]*\)', '', filename)
    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ï¼ˆ...ï¼‰ ã¾ãŸã¯ ï¼ˆ...ï¼‰ï¼ˆå…¨è§’æ‹¬å¼§ã®ã¿ï¼‰
    filename = re.sub(r'[_\sã€€]ï¼ˆ[^ï¼‰]*ï¼‰', '', filename)
    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã¾ãŸã¯ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ + åŠè§’é–‹ãæ‹¬å¼§ + ä»»æ„ã®æ–‡å­— + å…¨è§’é–‰ã˜æ‹¬å¼§ï¼ˆæ··åœ¨ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    filename = re.sub(r'[_\sã€€]\([^ï¼‰]*ï¼‰', '', filename)
    # ãƒ‘ã‚¿ãƒ¼ãƒ³4: å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã¾ãŸã¯ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ + å…¨è§’é–‹ãæ‹¬å¼§ + ä»»æ„ã®æ–‡å­— + åŠè§’é–‰ã˜æ‹¬å¼§ï¼ˆæ··åœ¨ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    filename = re.sub(r'[_\sã€€]ï¼ˆ[^)]*\)', '', filename)
    # ãƒ‘ã‚¿ãƒ¼ãƒ³5: æœ«å°¾ã®ç©ºç™½ã‚’å‰Šé™¤
    filename = filename.strip()
    return filename


def calculate_api_cost(prompt_tokens: int, completion_tokens: int) -> float:
    """
    APIä½¿ç”¨æ–™é‡‘ã‚’è¨ˆç®—ï¼ˆGPT-4o-miniï¼‰
    
    Args:
        prompt_tokens: å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        completion_tokens: å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        
    Returns:
        æ¨å®šè²»ç”¨ï¼ˆãƒ‰ãƒ«ï¼‰
    """
    INPUT_COST_PER_MILLION = 0.15  # $0.15 per 1M tokens
    OUTPUT_COST_PER_MILLION = 0.60  # $0.60 per 1M tokens
    
    input_cost = (prompt_tokens / 1_000_000) * INPUT_COST_PER_MILLION
    output_cost = (completion_tokens / 1_000_000) * OUTPUT_COST_PER_MILLION
    
    return input_cost + output_cost


def create_download_link(data: bytes, filename: str, mime_type: str) -> str:
    """
    Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ãŸãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ã‚’ä½œæˆ
    
    Args:
        data: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒã‚¤ãƒˆï¼‰
        filename: ãƒ•ã‚¡ã‚¤ãƒ«å
        mime_type: MIMEã‚¿ã‚¤ãƒ—
        
    Returns:
        HTMLãƒªãƒ³ã‚¯æ–‡å­—åˆ—
    """
    b64 = base64.b64encode(data).decode()
    href = f'<a href="data:{mime_type};base64,{b64}" download="{filename}" style="color: #1f77b4; text-decoration: underline; font-weight: bold;">ğŸ“¥ {filename}</a>'
    return href


def generate_completed_csv(df: pd.DataFrame, stats: Dict) -> str:
    """
    åˆ†æçµæœCSVå½¢å¼ã§å‡ºåŠ›ã™ã‚‹é–¢æ•°
    
    Args:
        df: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆé…ä¿¡æ™‚é–“, username, original_text, ãƒãƒ£ãƒƒãƒˆã®å±æ€§, ãƒãƒ£ãƒƒãƒˆæ„Ÿæƒ…ã‚’å«ã‚€ï¼‰
        stats: çµ±è¨ˆæƒ…å ±
        
    Returns:
        åˆ†æçµæœCSVæ–‡å­—åˆ—
    """
    # çµ±è¨ˆæƒ…å ±ã‚’CSVå½¢å¼ã®æ–‡å­—åˆ—ã¨ã—ã¦ä½œæˆ
    stats_lines = []
    
    # 1è¡Œç›®: çµ±è¨ˆæƒ…å ±,ä»¶æ•°
    stats_lines.append("çµ±è¨ˆæƒ…å ±,ä»¶æ•°")
    
    # 2è¡Œç›®: å…¨ã‚³ãƒ¡ãƒ³ãƒˆä»¶æ•°,{ä»¶æ•°}
    stats_lines.append(f"å…¨ã‚³ãƒ¡ãƒ³ãƒˆä»¶æ•°,{stats.get('total_comments', 0)}")
    
    # ç©ºè¡Œ
    stats_lines.append("")
    
    # 4è¡Œç›®: å±æ€§,ä»¶æ•°,,ãƒãƒ£ãƒƒãƒˆæ„Ÿæƒ…åˆ¥ä»¶æ•°,ä»¶æ•°,,ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆæ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°,ã‚³ãƒ¡ãƒ³ãƒˆæ•°
    stats_lines.append("å±æ€§,ä»¶æ•°,,ãƒãƒ£ãƒƒãƒˆæ„Ÿæƒ…åˆ¥ä»¶æ•°,ä»¶æ•°,,ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆæ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°,ã‚³ãƒ¡ãƒ³ãƒˆæ•°")
    
    # å±æ€§åˆ¥ä»¶æ•°ã€æ„Ÿæƒ…åˆ¥ä»¶æ•°ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’å–å¾—
    from config import CHAT_ATTRIBUTES, CHAT_SENTIMENTS
    attribute_counts = stats.get('attribute_counts', {})
    sentiment_counts = stats.get('sentiment_counts', {})
    
    # ã™ã¹ã¦ã®å±æ€§ã‚«ãƒ†ã‚´ãƒªã‚’å«ã‚€è¾æ›¸ã‚’ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„ã‚‚ã®ã¯0ï¼‰
    all_attribute_counts = {}
    for attr in CHAT_ATTRIBUTES:
        all_attribute_counts[attr] = attribute_counts.get(attr, 0)
    
    # ã™ã¹ã¦ã®æ„Ÿæƒ…ã‚«ãƒ†ã‚´ãƒªã‚’å«ã‚€è¾æ›¸ã‚’ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„ã‚‚ã®ã¯0ï¼‰
    all_sentiment_counts = {}
    for sent in CHAT_SENTIMENTS:
        all_sentiment_counts[sent] = sentiment_counts.get(sent, 0)
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆæ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆä¸Šä½10åï¼‰
    user_counts = {}
    if 'username' in df.columns:
        user_counts = df['username'].value_counts().head(10).to_dict()
    
    # æœ€å¤§è¡Œæ•°ã‚’è¨ˆç®—ï¼ˆå±æ€§ã€æ„Ÿæƒ…ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®æœ€å¤§å€¤ï¼‰
    max_rows = max(
        len(all_attribute_counts),
        len(all_sentiment_counts),
        len(user_counts),
        1  # æœ€å°1è¡Œ
    )
    
    # å±æ€§ã€æ„Ÿæƒ…ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›ï¼ˆé †åºä¿æŒï¼‰
    attr_items = list(all_attribute_counts.items())
    sentiment_items = list(all_sentiment_counts.items())
    user_items = list(user_counts.items())
    
    # ãƒ‡ãƒ¼ã‚¿è¡Œã‚’ç”Ÿæˆï¼ˆæ¨ªä¸¦ã³å½¢å¼ï¼‰
    for i in range(max_rows):
        # å±æ€§
        if i < len(attr_items):
            attr_name, attr_count = attr_items[i]
            attr_part = f"{attr_name},{attr_count}"
        else:
            attr_part = ","
        
        # ç©ºç™½åˆ—
        empty_col = ""
        
        # ãƒãƒ£ãƒƒãƒˆæ„Ÿæƒ…åˆ¥ä»¶æ•°
        if i < len(sentiment_items):
            sent_name, sent_count = sentiment_items[i]
            sentiment_part = f"{sent_name},{sent_count}"
        else:
            sentiment_part = ","
        
        # ç©ºç™½åˆ—
        empty_col2 = ""
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆæ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        if i < len(user_items):
            user_name, user_count = user_items[i]
            user_part = f"{user_name},{user_count}"
        else:
            user_part = ","
        
        # 1è¡Œã«çµåˆ
        row = f"{attr_part},,{sentiment_part},,{user_part}"
        stats_lines.append(row)
    
    # ç©ºè¡Œã‚’è¿½åŠ 
    stats_lines.append("")
    stats_lines.append("")
    
    # ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    stats_lines.append("ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿")
    
    # å¿…è¦ãªåˆ—ã®ã¿ã‚’é¸æŠï¼ˆé…ä¿¡æ™‚é–“, username, original_text, ãƒãƒ£ãƒƒãƒˆã®å±æ€§, ãƒãƒ£ãƒƒãƒˆæ„Ÿæƒ…ï¼‰
    output_columns = ['é…ä¿¡æ™‚é–“', 'username', 'original_text', 'ãƒãƒ£ãƒƒãƒˆã®å±æ€§', 'ãƒãƒ£ãƒƒãƒˆæ„Ÿæƒ…']
    available_columns = [col for col in output_columns if col in df.columns]
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å¿…è¦ãªåˆ—ã®ã¿ã«çµã‚‹
    output_df = df[available_columns].copy()
    
    # åˆ—åã‚’ç¢ºèªã—ã€é…ä¿¡æ™‚é–“ãŒãªã„å ´åˆã¯ inserted_at ã‚’ä½¿ç”¨
    if 'é…ä¿¡æ™‚é–“' not in output_df.columns and 'inserted_at' in df.columns:
        output_df['é…ä¿¡æ™‚é–“'] = df['inserted_at']
        output_df = output_df[output_columns]
    
    # é…ä¿¡æ™‚é–“ã§æ˜‡é †ã‚½ãƒ¼ãƒˆ
    if 'é…ä¿¡æ™‚é–“' in output_df.columns:
        # é…ä¿¡æ™‚é–“ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ã‚½ãƒ¼ãƒˆï¼ˆHH:MMå½¢å¼ã€å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚HH:MM:SSã«ã‚‚å¯¾å¿œï¼‰
        def parse_time(time_str):
            try:
                parts = str(time_str).split(':')
                if len(parts) >= 3:
                    hours, minutes, seconds = int(parts[0]), int(parts[1]), int(parts[2])
                    return hours * 3600 + minutes * 60 + seconds
                elif len(parts) == 2:
                    hours, minutes = int(parts[0]), int(parts[1])
                    return hours * 3600 + minutes * 60
                return 0
            except (ValueError, IndexError):
                return 0
        
        output_df['_sort_time'] = output_df['é…ä¿¡æ™‚é–“'].apply(parse_time)
        output_df = output_df.sort_values('_sort_time', ascending=True)
        output_df = output_df.drop(columns=['_sort_time'])
    
    # çµ±è¨ˆæƒ…å ±ã‚’CSVæ–‡å­—åˆ—ã«å¤‰æ›
    stats_csv = "\n".join(stats_lines)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’CSVæ–‡å­—åˆ—ã«å¤‰æ›
    data_csv = output_df.to_csv(index=False)
    
    # çµ±è¨ˆæƒ…å ±ã¨ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    combined_csv = stats_csv + "\n" + data_csv
    
    return combined_csv


def add_statistics_to_csv(df: pd.DataFrame, stats: Dict, is_question: bool = False, question_stats: Optional[Dict] = None) -> str:
    """
    CSVã«çµ±è¨ˆæƒ…å ±ã‚’è¿½åŠ ï¼ˆã‚°ãƒ©ãƒ•ä½œæˆã—ã‚„ã™ã„ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼‰
    
    Args:
        df: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        stats: çµ±è¨ˆæƒ…å ±
        is_question: è³ªå•CSVã‹ã©ã†ã‹
        question_stats: è³ªå•çµ±è¨ˆæƒ…å ±ï¼ˆè³ªå•CSVã®å ´åˆã®ã¿ï¼‰
        
    Returns:
        çµ±è¨ˆæƒ…å ±ãŒè¿½åŠ ã•ã‚ŒãŸCSVæ–‡å­—åˆ—
    """
    # çµ±è¨ˆæƒ…å ±ã‚’CSVå½¢å¼ã®æ–‡å­—åˆ—ã¨ã—ã¦ä½œæˆ
    stats_lines = []
    
    if is_question and question_stats:
        # è³ªå•CSVç”¨ã®çµ±è¨ˆæƒ…å ±
        stats_lines.append("çµ±è¨ˆæƒ…å ±")
        stats_lines.append(f"è³ªå•ã‚³ãƒ¡ãƒ³ãƒˆä»¶æ•°,{question_stats.get('total_questions', 0)}")
        stats_lines.append(f"è³ªå•å›ç­”ç‡,{question_stats.get('answer_rate', 0.0):.1f}%")
        stats_lines.append("")  # ç©ºè¡Œ
        stats_lines.append("è³ªå•ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿")
    else:
        # ãƒ¡ã‚¤ãƒ³CSVç”¨ã®çµ±è¨ˆæƒ…å ±
        stats_lines.append("çµ±è¨ˆæƒ…å ±")
        stats_lines.append(f"å…¨ã‚³ãƒ¡ãƒ³ãƒˆä»¶æ•°,{stats.get('total_comments', 0)}")
        stats_lines.append("")  # ç©ºè¡Œ
        stats_lines.append("ãƒãƒ£ãƒƒãƒˆã®å±æ€§åˆ¥ä»¶æ•°")
        stats_lines.append("å±æ€§,ä»¶æ•°")
        for attr, count in stats.get('attribute_counts', {}).items():
            stats_lines.append(f"{attr},{count}")
        stats_lines.append("")  # ç©ºè¡Œ
        stats_lines.append("ãƒãƒ£ãƒƒãƒˆæ„Ÿæƒ…åˆ¥ä»¶æ•°")
        stats_lines.append("æ„Ÿæƒ…,ä»¶æ•°")
        for sentiment, count in stats.get('sentiment_counts', {}).items():
            stats_lines.append(f"{sentiment},{count}")
        stats_lines.append("")  # ç©ºè¡Œ
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆæ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆä¸Šä½10åï¼‰
        if 'username' in df.columns:
            user_counts = df['username'].value_counts().head(10)
            stats_lines.append("ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆæ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
            stats_lines.append("ãƒ¦ãƒ¼ã‚¶ãƒ¼å,ã‚³ãƒ¡ãƒ³ãƒˆæ•°")
            for username, count in user_counts.items():
                stats_lines.append(f"{username},{count}")
            stats_lines.append("")  # ç©ºè¡Œ
        
        stats_lines.append("ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿")
    
    # çµ±è¨ˆæƒ…å ±ã‚’CSVæ–‡å­—åˆ—ã«å¤‰æ›
    stats_csv = "\n".join(stats_lines)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’CSVæ–‡å­—åˆ—ã«å¤‰æ›
    data_csv = df.to_csv(index=False)
    
    # çµ±è¨ˆæƒ…å ±ã¨ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    combined_csv = stats_csv + "\n" + data_csv
    
    return combined_csv


def format_remaining_time(seconds: float) -> str:
    """
    æ®‹ã‚Šæ™‚é–“ï¼ˆç§’ï¼‰ã‚’ã€Œã‚ã¨â—¯åˆ†â—¯ç§’ã€å½¢å¼ã«å¤‰æ›
    
    Args:
        seconds: æ®‹ã‚Šæ™‚é–“ï¼ˆç§’ï¼‰
        
    Returns:
        ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸæ®‹ã‚Šæ™‚é–“ã®æ–‡å­—åˆ—
    """
    if seconds < 0:
        return "ã‚ã¨0ç§’"
    
    total_seconds = int(seconds)
    
    # 1æ™‚é–“ä»¥ä¸Šã®å ´åˆ
    if total_seconds >= 3600:
        hours = total_seconds // 3600
        minutes = (total_seconds % 3600) // 60
        return f"ã‚ã¨{hours}æ™‚é–“{minutes}åˆ†"
    
    # 1åˆ†ä»¥ä¸Š1æ™‚é–“æœªæº€ã®å ´åˆ
    elif total_seconds >= 60:
        minutes = total_seconds // 60
        secs = total_seconds % 60
        return f"ã‚ã¨{minutes}åˆ†{secs}ç§’"
    
    # 1åˆ†æœªæº€ã®å ´åˆ
    else:
        return f"ã‚ã¨{total_seconds}ç§’"


def main():
    st.set_page_config(
        page_title="ãƒ©ã‚¤ãƒ–é…ä¿¡ãƒãƒ£ãƒƒãƒˆåˆ†æãƒ„ãƒ¼ãƒ«",
        page_icon="ğŸ“Š",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # ã‚«ã‚¹ã‚¿ãƒ CSSã‚’æ³¨å…¥
    inject_custom_css()

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼: APIã‚­ãƒ¼è¨­å®š
    with st.sidebar:
        has_api_key = render_api_key_input()
        st.divider()

    # APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã®è­¦å‘Š
    if not has_api_key:
        st.warning("åˆ†æã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯APIã‚­ãƒ¼ã®è¨­å®šãŒå¿…è¦ã§ã™ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        st.info("[OpenAI APIã‚­ãƒ¼ã®å–å¾—ã¯ã“ã¡ã‚‰](https://platform.openai.com/api-keys)")
        st.stop()

    # ã‚³ãƒ¡ãƒ³ãƒˆåˆ†ææ©Ÿèƒ½ã®ãƒšãƒ¼ã‚¸ã‚’è¡¨ç¤º
    show_comment_analysis_page()


def show_comment_analysis_page():
    """ã‚³ãƒ¡ãƒ³ãƒˆåˆ†ææ©Ÿèƒ½ã®ãƒšãƒ¼ã‚¸ã‚’è¡¨ç¤º"""
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
    if "processed_data" not in st.session_state:
        st.session_state.processed_data = None
    if "analysis_complete" not in st.session_state:
        st.session_state.analysis_complete = False
    if "analysis_save_path" not in st.session_state:
        st.session_state.analysis_save_path = None
    if "analysis_original_df" not in st.session_state:
        st.session_state.analysis_original_df = None
    if "analysis_cancelled" not in st.session_state:
        st.session_state.analysis_cancelled = False
    if "csv_completed_data" not in st.session_state:
        st.session_state.csv_completed_data = None
    if "csv_completed_filename" not in st.session_state:
        st.session_state.csv_completed_filename = None
    if "stats_data" not in st.session_state:
        st.session_state.stats_data = None
    if "question_stats_data" not in st.session_state:
        st.session_state.question_stats_data = None
    if "question_df_data" not in st.session_state:
        st.session_state.question_df_data = None
    if "uploaded_csv_filename" not in st.session_state:
        st.session_state.uploaded_csv_filename = ""
    if "api_usage" not in st.session_state:
        st.session_state.api_usage = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
            "estimated_cost_usd": 0.0
        }
    if "selected_company" not in st.session_state:
        st.session_state.selected_company = DEFAULT_COMPANY
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼: APIä½¿ç”¨çŠ¶æ³ï¼ˆåˆ†æå®Œäº†æ™‚ã®ã¿è¡¨ç¤ºï¼‰
    with st.sidebar:
        if st.session_state.get("analysis_complete") and st.session_state.get("api_usage") and st.session_state.api_usage["total_tokens"] > 0:
            st.divider()
            st.subheader("APIä½¿ç”¨çŠ¶æ³")
            usage = st.session_state.api_usage
            st.metric("ä½¿ç”¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°", f"{usage['total_tokens']:,}")
            st.write(f"å…¥åŠ›: {usage['prompt_tokens']:,} ãƒˆãƒ¼ã‚¯ãƒ³")
            st.write(f"å‡ºåŠ›: {usage['completion_tokens']:,} ãƒˆãƒ¼ã‚¯ãƒ³")
            st.metric("æ¨å®šè²»ç”¨", f"${usage['estimated_cost_usd']:.4f}")
            st.caption("ãƒ¢ãƒ‡ãƒ«: GPT-4o Mini")
    
    st.title("ãƒ©ã‚¤ãƒ–é…ä¿¡ãƒãƒ£ãƒƒãƒˆåˆ†æãƒ„ãƒ¼ãƒ«")
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    st.header("1. CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    st.info("ğŸ’¡ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°ã‚¢ãƒ³ãƒ‰ãƒ‰ãƒ­ãƒƒãƒ—ã™ã‚‹ã‹ã€ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
    uploaded_file = st.file_uploader(
        "ğŸ“„ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—å¯ï¼‰",
        type=["csv"],
        help="CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°ã‚¢ãƒ³ãƒ‰ãƒ‰ãƒ­ãƒƒãƒ—ã™ã‚‹ã‹ã€ã‚¯ãƒªãƒƒã‚¯ã—ã¦é¸æŠã—ã¦ãã ã•ã„ã€‚å¿…è¦ãªåˆ—: guest_id, username, original_text, inserted_at"
    )
    
    if uploaded_file is not None:
        try:
            # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä¿å­˜ï¼ˆæ‹¡å¼µå­ãªã—ï¼‰
            uploaded_filename = uploaded_file.name
            if uploaded_filename.endswith('.csv'):
                uploaded_filename_base = uploaded_filename[:-4]  # .csvã‚’é™¤å»
            else:
                uploaded_filename_base = uploaded_filename
            # ãƒ©ã‚¤ãƒ–é…ä¿¡åã‚’å‰Šé™¤
            uploaded_filename_base = remove_live_name_from_filename(uploaded_filename_base)
            st.session_state.uploaded_csv_filename = uploaded_filename_base
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with tempfile.NamedTemporaryFile(delete=False, suffix=".csv") as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                tmp_path = tmp_file.name
            
            # CSVã‚’èª­ã¿è¾¼ã‚“ã§å‡¦ç†
            with st.spinner("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™..."):
                # elapsed_timeã‚«ãƒ©ãƒ ãŒã‚ã‚‹ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
                try:
                    # ã¾ãšãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§elapsed_timeã‚«ãƒ©ãƒ ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    test_df = pd.read_csv(tmp_path, encoding='utf-8-sig', nrows=1)
                    has_elapsed_time = 'elapsed_time' in test_df.columns
                    
                    if has_elapsed_time:
                        # elapsed_timeã‚«ãƒ©ãƒ ãŒã‚ã‚‹å ´åˆã¯æ–°ã—ã„å‡¦ç†ã‚’ä½¿ç”¨
                        df = load_csv_with_elapsed_time(tmp_path)
                    else:
                        # elapsed_timeã‚«ãƒ©ãƒ ãŒãªã„å ´åˆã¯æ—¢å­˜ã®å‡¦ç†ã‚’ä½¿ç”¨
                        df = load_csv(tmp_path)
                        df = validate_and_process_data(df)
                except Exception as e:
                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯æ—¢å­˜ã®å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    st.warning(f"elapsed_timeã‚«ãƒ©ãƒ ã®æ¤œå‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚æ—¢å­˜ã®å‡¦ç†ã‚’ä½¿ç”¨ã—ã¾ã™: {str(e)}")
                    df = load_csv(tmp_path)
                    df = validate_and_process_data(df)
                
                st.session_state.processed_data = df
                st.session_state.analysis_complete = False
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            st.success(f"âœ“ {len(df)}ä»¶ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            st.subheader("ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
            st.dataframe(df.head(10), use_container_width=True)
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            os.unlink(tmp_path)
            
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return
    
    # ä¼æ¥­é¸æŠï¼ˆãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ã«ç§»å‹•ï¼‰
    st.header("2. ä¼æ¥­é¸æŠ")
    company_names = list(COMPANIES.keys())
    
    selected_company = st.selectbox(
        "ä¼æ¥­ã‚’é¸æŠã—ã¦ãã ã•ã„",
        company_names,
        index=company_names.index(st.session_state.selected_company) if st.session_state.selected_company in company_names else 0
    )
    
    # ä¼æ¥­é¸æŠãŒå¤‰æ›´ã•ã‚ŒãŸå ´åˆã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‚’æ›´æ–°
    if selected_company != st.session_state.selected_company:
        st.session_state.selected_company = selected_company
        # åˆ†æçµæœã‚’ã‚¯ãƒªã‚¢ï¼ˆä¼æ¥­ãŒå¤‰ã‚ã£ãŸã‚‰å†åˆ†æãŒå¿…è¦ï¼‰
        if "analysis_complete" in st.session_state:
            st.session_state.analysis_complete = False
        # æ³¨æ„: processed_dataã¯ä¿æŒã™ã‚‹ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®CSVãƒ‡ãƒ¼ã‚¿ã¯æ®‹ã™ï¼‰
        # åˆ†æçµæœã ã‘ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹ãŸã‚ã€stats_dataãªã©ã‚‚ã‚¯ãƒªã‚¢
        if "stats_data" in st.session_state:
            st.session_state.stats_data = None
        if "question_stats_data" in st.session_state:
            st.session_state.question_stats_data = None
        if "question_df_data" in st.session_state:
            st.session_state.question_df_data = None
        if "csv_completed_data" in st.session_state:
            st.session_state.csv_completed_data = None
    
    # ç¾åœ¨ã®ä¼æ¥­è¨­å®šã‚’å–å¾—
    company_config = get_company_config(selected_company)
    st.info(f"**é¸æŠä¸­ã®ä¼æ¥­**: {company_config['name']}")
    
    # AIåˆ†æ
    if st.session_state.processed_data is not None and not st.session_state.analysis_complete:
        st.header("3. AIåˆ†æ")
        
        df = st.session_state.processed_data.copy()
        
        # åˆ†æé€”ä¸­ã®çµæœãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆPCã‚¹ãƒªãƒ¼ãƒ—å¯¾ç­–ï¼‰
        analysis_resume_available = False
        saved_count = 0
        
        # ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æ¤œç´¢ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ãªã„å ´åˆã§ã‚‚æ¤œç´¢ï¼‰
        if not st.session_state.analysis_save_path:
            # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æœ€æ–°ã®ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            save_dir = tempfile.gettempdir()
            save_files = glob.glob(os.path.join(save_dir, "analysis_save_*.pkl"))
            if save_files:
                # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
                latest_file = max(save_files, key=os.path.getmtime)
                st.session_state.analysis_save_path = latest_file
        
        if st.session_state.analysis_save_path and os.path.exists(st.session_state.analysis_save_path):
            try:
                with open(st.session_state.analysis_save_path, 'rb') as f:
                    saved_data = pickle.load(f)
                    if saved_data:
                        if isinstance(saved_data, list):
                            saved_count = len(saved_data)
                        elif isinstance(saved_data, pd.DataFrame):
                            saved_count = len(saved_data)
                        if saved_count > 0:
                            analysis_resume_available = True
            except Exception:
                pass
        
        if analysis_resume_available:
            st.warning(f"âš ï¸ åˆ†æãŒé€”ä¸­ã§ä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚{saved_count}ä»¶ã®åˆ†æçµæœãŒä¿å­˜ã•ã‚Œã¦ã„ã¾ã™ã€‚ç¶šãã‹ã‚‰å†é–‹ã§ãã¾ã™ã€‚")
            col1, col2 = st.columns(2)
            with col1:
                if st.button("ç¶šãã‹ã‚‰å†é–‹", type="primary"):
                    st.session_state.analysis_resume = True
                    # å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç¢ºä¿
                    if st.session_state.analysis_original_df is None:
                        st.session_state.analysis_original_df = df.copy()
                    st.rerun()
            with col2:
                if st.button("æœ€åˆã‹ã‚‰é–‹å§‹"):
                    # ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                    if st.session_state.analysis_save_path and os.path.exists(st.session_state.analysis_save_path):
                        try:
                            os.remove(st.session_state.analysis_save_path)
                        except Exception:
                            pass
                    st.session_state.analysis_resume = False
                    st.session_state.analysis_save_path = None
                    st.session_state.analysis_original_df = None
                    st.rerun()
        
        # åˆ†æé–‹å§‹ãƒ»ä¸­æ–­ãƒœã‚¿ãƒ³
        col1, col2 = st.columns(2)
        with col1:
            start_analysis = st.button("åˆ†æã‚’é–‹å§‹", type="primary")
        with col2:
            cancel_analysis = st.button("åˆ†æã‚’ä¸­æ–­", type="secondary", disabled=st.session_state.analysis_complete)
        
        # ä¸­æ–­ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸå ´åˆ
        if cancel_analysis:
            st.session_state.analysis_cancelled = True
            st.warning("âš ï¸ åˆ†æã®ä¸­æ–­ã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆã—ã¾ã—ãŸã€‚ç¾åœ¨å‡¦ç†ä¸­ã®ã‚³ãƒ¡ãƒ³ãƒˆãŒå®Œäº†æ¬¡ç¬¬ã€åˆ†æãŒä¸­æ–­ã•ã‚Œã¾ã™ã€‚")
        
        # åˆ†æé–‹å§‹
        if start_analysis or st.session_state.get("analysis_resume", False):
            # APIã‚­ãƒ¼äº‹å‰ãƒã‚§ãƒƒã‚¯
            try:
                from config import get_openai_api_key
                if not get_openai_api_key():
                    st.error("OpenAI APIã‚­ãƒ¼ãŒæœªè¨­å®šã§ã™ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰è¨­å®šã—ã¦ãã ã•ã„ã€‚")
                    return
            except Exception:
                st.error("APIã‚­ãƒ¼ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å†åº¦ã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                return

            # ä¸­æ–­ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
            st.session_state.analysis_cancelled = False
            # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆæ–°ã—ã„åˆ†æé–‹å§‹æ™‚ã®ã¿ï¼‰
            if start_analysis:
                st.session_state.api_usage = {
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_tokens": 0,
                    "estimated_cost_usd": 0.0
                }
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’è¨­å®šï¼ˆPCã‚¹ãƒªãƒ¼ãƒ—å¯¾ç­–ï¼‰
            if not st.session_state.analysis_save_path:
                save_dir = tempfile.gettempdir()
                save_filename = f"analysis_save_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
                st.session_state.analysis_save_path = os.path.join(save_dir, save_filename)
            
            # å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä¿å­˜ï¼ˆå†é–‹æ™‚ã«ä½¿ç”¨ï¼‰
            if st.session_state.analysis_original_df is None:
                st.session_state.analysis_original_df = df.copy()
            else:
                df = st.session_state.analysis_original_df.copy()
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²
            start_time = time.time()
            
            def update_progress(current, total):
                progress = current / total
                progress_bar.progress(progress)
                
                # çµŒéæ™‚é–“ã®è¨ˆç®—
                elapsed_time = time.time() - start_time
                elapsed_seconds = int(elapsed_time)
                hours = elapsed_seconds // 3600
                minutes = (elapsed_seconds % 3600) // 60
                seconds = elapsed_seconds % 60
                elapsed_str = f"{hours:02d}:{minutes:02d}:{seconds:02d}"
                
                # äºˆæƒ³å®Œäº†æ™‚é–“ã®è¨ˆç®—
                if current > 0:
                    avg_time_per_item = elapsed_time / current
                    remaining_items = total - current
                    estimated_remaining = avg_time_per_item * remaining_items
                    estimated_str = format_remaining_time(estimated_remaining)
                    
                    status_text.text(
                        f"é€²è¡Œä¸­: {current}/{total} ({progress*100:.1f}%)\n"
                        f"çµŒéæ™‚é–“: {elapsed_str}\n"
                        f"äºˆæƒ³å®Œäº†æ™‚é–“: {estimated_str}"
                    )
                else:
                    status_text.text(f"é€²è¡Œä¸­: {current}/{total} ({progress*100:.1f}%)")
            
            def save_intermediate_results(action, results=None):
                """ä¸­é–“çµæœã‚’ä¿å­˜ï¼ˆPCã‚¹ãƒªãƒ¼ãƒ—å¯¾ç­–ï¼‰"""
                save_path = st.session_state.analysis_save_path
                
                if action == "save" and results is not None:
                    # çµæœã‚’ä¿å­˜
                    try:
                        with open(save_path, 'wb') as f:
                            pickle.dump(results, f)
                    except Exception as e:
                        print(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
                elif action == "load":
                    # ä¿å­˜ã•ã‚ŒãŸçµæœã‚’èª­ã¿è¾¼ã‚€
                    if save_path and os.path.exists(save_path):
                        try:
                            with open(save_path, 'rb') as f:
                                saved_results = pickle.load(f)
                                return saved_results
                        except Exception as e:
                            print(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                    return None
                elif action == "clear":
                    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                    if save_path and os.path.exists(save_path):
                        try:
                            os.remove(save_path)
                        except Exception:
                            pass
            
            def check_cancel():
                """ä¸­æ–­ãƒ•ãƒ©ã‚°ã‚’ãƒã‚§ãƒƒã‚¯"""
                return st.session_state.get("analysis_cancelled", False)
            
            try:
                # AIåˆ†æå®Ÿè¡Œï¼ˆçµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½¿ç”¨ï¼š50%é«˜é€ŸåŒ–ï¼‰
                with st.spinner("AIåˆ†æã‚’å®Ÿè¡Œä¸­ã§ã™ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„..."):
                    analysis_result = analyze_all_comments(df, update_progress, save_intermediate_results, check_cancel)
                
                # åˆ†æçµæœã‹ã‚‰DataFrameã¨ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡æƒ…å ±ã‚’å–å¾—
                if isinstance(analysis_result, dict):
                    analyzed_df = analysis_result["df"]
                    api_usage_info = analysis_result.get("api_usage", {})
                else:
                    # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã€DataFrameãŒç›´æ¥è¿”ã•ã‚ŒãŸå ´åˆ
                    analyzed_df = analysis_result
                    api_usage_info = {}
                
                st.session_state.processed_data = analyzed_df
                st.session_state.analysis_complete = True
                st.session_state.analysis_resume = False
                st.session_state.analysis_original_df = None
                st.session_state.analysis_cancelled = False  # å®Œäº†æ™‚ã«ä¸­æ–­ãƒ•ãƒ©ã‚°ã‚’ã‚¯ãƒªã‚¢
                
                # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡æƒ…å ±ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜
                if api_usage_info:
                    prompt_tokens = api_usage_info.get("prompt_tokens", 0)
                    completion_tokens = api_usage_info.get("completion_tokens", 0)
                    total_tokens = api_usage_info.get("total_tokens", 0)
                    estimated_cost = calculate_api_cost(prompt_tokens, completion_tokens)
                    
                    st.session_state.api_usage = {
                        "prompt_tokens": prompt_tokens,
                        "completion_tokens": completion_tokens,
                        "total_tokens": total_tokens,
                        "estimated_cost_usd": estimated_cost
                    }
                    
                    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºåŠ›ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ãŒ0ã®å ´åˆã®åŸå› ç‰¹å®šç”¨ï¼‰
                    if total_tokens == 0:
                        st.warning(f"âš ï¸ ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ãŒ0ã§ã™ã€‚åˆ†æã•ã‚ŒãŸã‚³ãƒ¡ãƒ³ãƒˆæ•°: {len(analyzed_df)}")
                else:
                    # api_usage_infoãŒç©ºã®å ´åˆã®è­¦å‘Š
                    st.warning("âš ï¸ APIä½¿ç”¨é‡æƒ…å ±ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                
                # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•ç”Ÿæˆï¼ˆåˆ†æå®Œäº†æ™‚ã«è‡ªå‹•å®Ÿè¡Œï¼‰
                try:
                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«åã‚’å«ã‚€ï¼‰
                    uploaded_filename_base = st.session_state.get("uploaded_csv_filename", "")
                    if uploaded_filename_base:
                        default_file_title = f"ã‚³ãƒ¡ãƒ³ãƒˆåˆ†æ_{uploaded_filename_base}"
                    else:
                        default_file_title = "ã‚³ãƒ¡ãƒ³ãƒˆåˆ†æ"
                    
                    # çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—ï¼ˆå¾Œã§CSVã«è¿½åŠ ã™ã‚‹ãŸã‚ï¼‰
                    temp_stats = calculate_statistics(analyzed_df)
                    
                    # åˆ†æçµæœCSVå½¢å¼ã§å‡ºåŠ›
                    try:
                        # åˆ†æçµæœCSVå½¢å¼ã§å‡ºåŠ›
                        completed_csv = generate_completed_csv(analyzed_df, temp_stats)
                        st.session_state.csv_completed_data = completed_csv.encode('utf-8-sig')
                        st.session_state.csv_completed_filename = f"{default_file_title}_åˆ†æçµæœ.csv"
                    except Exception as e:
                        # åˆ†æçµæœCSVç”Ÿæˆã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ï¼ˆå¾Œã§å†ç”Ÿæˆå¯èƒ½ï¼‰
                        print(f"åˆ†æçµæœCSVç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                except Exception as e:
                    # CSVç”Ÿæˆã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ï¼ˆå¾Œã§å†ç”Ÿæˆå¯èƒ½ï¼‰
                    print(f"CSVè‡ªå‹•ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                
                # çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—ã—ã¦ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜
                question_df = extract_questions(analyzed_df)
                question_df["å›ç­”çŠ¶æ³"] = "æœªå›ç­”"
                st.session_state.stats_data = calculate_statistics(analyzed_df)
                st.session_state.question_stats_data = calculate_question_statistics(question_df)
                st.session_state.question_df_data = question_df
                
                progress_bar.progress(1.0)
                status_text.text("âœ“ åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                st.success("åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                
                # é€šçŸ¥éŸ³ã‚’å†ç”Ÿ
                st.components.v1.html("""
                <script>
                // ãƒ“ãƒ¼ãƒ—éŸ³ã‚’å†ç”Ÿã™ã‚‹é–¢æ•°
                function playBeep() {
                    const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    const oscillator = audioContext.createOscillator();
                    const gainNode = audioContext.createGain();
                    
                    oscillator.connect(gainNode);
                    gainNode.connect(audioContext.destination);
                    
                    oscillator.frequency.value = 800; // å‘¨æ³¢æ•°ï¼ˆHzï¼‰
                    oscillator.type = 'sine';
                    
                    gainNode.gain.setValueAtTime(0.3, audioContext.currentTime);
                    gainNode.gain.exponentialRampToValueAtTime(0.01, audioContext.currentTime + 0.5);
                    
                    oscillator.start(audioContext.currentTime);
                    oscillator.stop(audioContext.currentTime + 0.5);
                }
                
                // éŸ³ã‚’å†ç”Ÿ
                playBeep();
                </script>
                """, height=0)
                
                # åˆ†æçµæœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
                st.subheader("åˆ†æçµæœãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
                st.dataframe(analyzed_df.head(10), use_container_width=True)
                
            except KeyboardInterrupt:
                # ä¸­æ–­ãŒãƒªã‚¯ã‚¨ã‚¹ãƒˆã•ã‚ŒãŸå ´åˆ
                st.session_state.analysis_cancelled = True
                st.warning("âš ï¸ åˆ†æãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚")
                st.info("ğŸ’¡ ã€Œç¶šãã‹ã‚‰å†é–‹ã€ãƒœã‚¿ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã€ä¸­æ–­ã—ãŸç®‡æ‰€ã‹ã‚‰åˆ†æã‚’å†é–‹ã§ãã¾ã™ã€‚")
                # ä¸­æ–­ãƒ•ãƒ©ã‚°ã‚’ã‚¯ãƒªã‚¢ã—ã¦ã€æ¬¡å›ã®å†é–‹æ™‚ã«å•é¡ŒãŒãªã„ã‚ˆã†ã«ã™ã‚‹
                st.session_state.analysis_cancelled = False
                st.rerun()
            except Exception as e:
                # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼
                error_message = str(e)
                if "ä¸­æ–­" in error_message or "KeyboardInterrupt" in error_message:
                    # ä¸­æ–­é–¢é€£ã®ã‚¨ãƒ©ãƒ¼ã®å ´åˆ
                    st.session_state.analysis_cancelled = True
                    st.warning("âš ï¸ åˆ†æãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚")
                    st.info("ğŸ’¡ ã€Œç¶šãã‹ã‚‰å†é–‹ã€ãƒœã‚¿ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã€ä¸­æ–­ã—ãŸç®‡æ‰€ã‹ã‚‰åˆ†æã‚’å†é–‹ã§ãã¾ã™ã€‚")
                    st.session_state.analysis_cancelled = False
                    st.rerun()
                else:
                    # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼
                    st.error(f"åˆ†æã‚¨ãƒ©ãƒ¼: {error_message}")
                    st.info("ğŸ’¡ PCãŒã‚¹ãƒªãƒ¼ãƒ—ã—ãŸå ´åˆã¯ã€ãƒšãƒ¼ã‚¸ã‚’ãƒªãƒ­ãƒ¼ãƒ‰ã—ã¦ã€Œç¶šãã‹ã‚‰å†é–‹ã€ãƒœã‚¿ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
                    import traceback
                    with st.expander("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±"):
                        st.code(traceback.format_exc())
                    return
    
    # ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›
    if st.session_state.analysis_complete and st.session_state.processed_data is not None:
        st.header("3. ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›")
        
        # çµ±è¨ˆæƒ…å ±ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‹ã‚‰å–å¾—ï¼ˆãªã‘ã‚Œã°è¨ˆç®—ï¼‰
        if st.session_state.stats_data is None:
            df = st.session_state.processed_data.copy()
            question_df = extract_questions(df)
            question_df["å›ç­”çŠ¶æ³"] = "æœªå›ç­”"
            st.session_state.stats_data = calculate_statistics(df)
            st.session_state.question_stats_data = calculate_question_statistics(question_df)
            st.session_state.question_df_data = question_df
        
        df = st.session_state.processed_data.copy()
        stats = st.session_state.stats_data
        question_stats = st.session_state.question_stats_data
        question_df = st.session_state.question_df_data
        
        # çµ±è¨ˆæƒ…å ±ã‚’å¸¸ã«è¡¨ç¤ºï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ã‚‚è¡¨ç¤ºã•ã‚Œã‚‹ï¼‰
        st.subheader("çµ±è¨ˆæƒ…å ±")
        
        stat_col1, stat_col2 = st.columns(2)
        
        with stat_col1:
            st.metric("å…¨ã‚³ãƒ¡ãƒ³ãƒˆä»¶æ•°", stats["total_comments"])
            st.write("**ãƒãƒ£ãƒƒãƒˆã®å±æ€§åˆ¥ä»¶æ•°**")
            for attr, count in stats["attribute_counts"].items():
                st.write(f"- {attr}: {count}ä»¶")
        
        with stat_col2:
            if len(question_df) > 0:
                # è³ªå•çµ±è¨ˆæƒ…å ±ãŒè¨ˆç®—ã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿è¡¨ç¤º
                if question_stats is not None and "total_questions" in question_stats:
                    st.metric("è³ªå•ã‚³ãƒ¡ãƒ³ãƒˆä»¶æ•°", question_stats["total_questions"])
                else:
                    st.metric("è³ªå•ã‚³ãƒ¡ãƒ³ãƒˆä»¶æ•°", len(question_df))
            else:
                st.info("è³ªå•ã‚³ãƒ¡ãƒ³ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            
            st.write("**ãƒãƒ£ãƒƒãƒˆæ„Ÿæƒ…åˆ¥ä»¶æ•°**")
            for sentiment, count in stats["sentiment_counts"].items():
                st.write(f"- {sentiment}: {count}ä»¶")
        
        st.markdown("---")
        
        # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ï¼ˆåˆ†æå®Œäº†æ™‚ã«è‡ªå‹•ç”Ÿæˆã•ã‚Œã‚‹ï¼‰
        st.subheader("ğŸ“¥ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å¤‰æ›´ã—ãŸã„å ´åˆã®å…¥åŠ›æ¬„ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        uploaded_filename_base = st.session_state.get("uploaded_csv_filename", "")
        if uploaded_filename_base:
            default_file_title = f"ã‚³ãƒ¡ãƒ³ãƒˆåˆ†æ_{uploaded_filename_base}"
        else:
            default_file_title = "ã‚³ãƒ¡ãƒ³ãƒˆåˆ†æ"
        
        file_title = st.text_input(
            "ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å¤‰æ›´ï¼ˆæ‹¡å¼µå­ãªã—ã€å¤‰æ›´ã—ãªã„å ´åˆã¯ãã®ã¾ã¾ï¼‰",
            value=default_file_title,
            key="csv_filename_input"
        )
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åãŒå¤‰æ›´ã•ã‚ŒãŸå ´åˆã¯ã€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†ç”Ÿæˆ
        if file_title and ("csv_completed_filename" not in st.session_state or 
                          not st.session_state.csv_completed_filename or 
                          file_title not in st.session_state.csv_completed_filename):
            try:
                # åˆ†æçµæœCSVã‚’å†ç”Ÿæˆ
                completed_csv = generate_completed_csv(df, stats)
                st.session_state.csv_completed_data = completed_csv.encode('utf-8-sig')
                st.session_state.csv_completed_filename = f"{file_title}_åˆ†æçµæœ.csv"
            except Exception as e:
                st.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # åˆ†æçµæœCSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯
        if "csv_completed_data" in st.session_state and st.session_state.csv_completed_data:
            download_link = create_download_link(
                st.session_state.csv_completed_data,
                st.session_state.csv_completed_filename,
                "text/csv"
            )
            st.markdown(f"**åˆ†æçµæœCSV**: {download_link}", unsafe_allow_html=True)
        else:
            # åˆ†æçµæœCSVãŒã¾ã ç”Ÿæˆã•ã‚Œã¦ã„ãªã„å ´åˆã€ç”Ÿæˆã‚’è©¦ã¿ã‚‹
            st.info("ğŸ’¡ åˆ†æçµæœCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆä¸­...")
            try:
                completed_csv = generate_completed_csv(df, stats)
                st.session_state.csv_completed_data = completed_csv.encode('utf-8-sig')
                uploaded_filename_base = st.session_state.get("uploaded_csv_filename", "")
                if uploaded_filename_base:
                    default_file_title = f"ã‚³ãƒ¡ãƒ³ãƒˆåˆ†æ_{uploaded_filename_base}"
                else:
                    default_file_title = "ã‚³ãƒ¡ãƒ³ãƒˆåˆ†æ"
                st.session_state.csv_completed_filename = f"{default_file_title}_åˆ†æçµæœ.csv"
                download_link = create_download_link(
                    st.session_state.csv_completed_data,
                    st.session_state.csv_completed_filename,
                    "text/csv"
                )
                st.markdown(f"**åˆ†æçµæœCSV**: {download_link}", unsafe_allow_html=True)
            except Exception as e:
                st.warning(f"åˆ†æçµæœCSVãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.markdown("---")
    # ãƒ•ã‚©ãƒ«ãƒ€åã‚’å–å¾—
    folder_name = os.path.basename(os.path.dirname(os.path.abspath(__file__)))
    st.markdown(
        f"""
        <div style='text-align: center; color: gray;'>
        <p>{folder_name}</p>
        </div>
        """,
        unsafe_allow_html=True
    )


if __name__ == "__main__":
    main()


